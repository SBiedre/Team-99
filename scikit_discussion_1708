# **1. What is an error rate?**
In simple terms, the error rate in the scikit-learn library is a measure of how often a machine learning model makes wrong predictions. It is calculated as the percentage of incorrect predictions out of the total number of predictions made. 
For example, if a model predicts 100 times and 10 of those predictions are wrong, the error rate would be 10%.

# **2. Where you could use other machine-learning models?**
Machine learning models can be used in various areas, including:
* Healthcare: Predicting diseases, personalizing treatments, and analyzing medical images.
* Finance: Detecting fraud, predicting stock prices, and assessing credit risk.
* Retail: Recommending products, optimizing pricing, and managing inventory.
* Transportation: Optimizing routes, predicting traffic, and enabling self-driving cars.
* Marketing: Segmenting customers, targeting ads, and analyzing customer sentiment.
* Manufacturing: Predictive maintenance, quality control, and optimizing production processes.

# **3. What is the difference between supervised and unsupervised training?**
The difference between supervised and unsupervised training lies in the type of data used and the goal of the training process:

Supervised Training:
  Data: Uses labeled data, meaning each training example comes with an input-output pair (features and the correct answer).
  Goal: The model learns to predict the output from the input data.
  Example: Predicting house prices based on features like size, location, and number of bedrooms.

Unsupervised Training:
   Data: Uses unlabeled data, meaning the training examples only have input data without any associated output.
   Goal: The model finds patterns or structures in the input data.
   Example: Grouping customers into segments based on purchasing behavior without prior knowledge of the segments.

# **4. How to import different models from the scikit-learn package?**
Just use "import" statement and specifying the model you want to use. For example ```from sklearn.tree import DecisionTreeClassifier```.

# **5. How can you evaluate the performance of a machine learning model in scikit-learn**
The evaluation technique depends on the type of model and problem (classification, regression, etc.) you use. There are various evaluation tools available, 
to use desired tool you need to impot it.
For classification models you can use:
- "accuracy_score" measures the percentage of correct predictions, 
- "confusion_matrix" shows the counts of true positives, true negatives, false positives, and false negatives,
- "precision_score", recall_score and f1_score provide more detailed insights, especially when dealing with imbalanced dataset,
- "roc_auc_score" measures the model's ability to distinguish between classes.
For regression models you can use:
- "mean_absolute_error" measures the average absolute errors between the predicted and actual values,
- "mean_squared_error" measures the average squared errors between predicted and actual values,
- "r2_score" represents the proportion of variance explained by the model.
Cross-Validation Score "cross_val_score" evaluates the model's performance on different subsets of the data to ensure it's not overfitting.

# **6. What metrics are commonly used for evaluation?**
Classification Metrics:
    Accuracy: The proportion of correctly predicted instances out of the total instances.
        Good for balanced datasets.
    Precision: The proportion of true positive predictions out of all positive predictions.
        Useful when the cost of false positives is high.
    Recall (Sensitivity): The proportion of true positive predictions out of all actual positives.
        Important when the cost of false negatives is high.
    F1-Score: The harmonic mean of precision and recall.
        Balances precision and recall, useful for imbalanced datasets.
    Confusion Matrix: A table that shows the true positives, true negatives, false positives, and false negatives.
        Provides detailed insight into the types of errors.
    ROC-AUC Score: The area under the Receiver Operating Characteristic curve, which plots the true positive rate against the false positive rate.
        Measures the model's ability to distinguish between classes.

Regression Metrics:
    Mean Absolute Error (MAE): The average absolute difference between predicted and actual values.
        Easy to interpret and less sensitive to outliers.
    Mean Squared Error (MSE): The average of the squared differences between predicted and actual values.
        Penalizes larger errors more, sensitive to outliers.
    Root Mean Squared Error (RMSE): The square root of MSE.
        Provides error in the same units as the target variable.
    R-squared (R²) Score: The proportion of the variance in the dependent variable that is predictable from the independent variables.
        Indicates the goodness of fit of the model.

Clustering Metrics:
    Silhouette Score: Measures how similar an object is to its own cluster compared to other clusters.
        Values range from -1 to 1; higher values indicate better-defined clusters.
    Inertia (Within-Cluster Sum of Squares): Measures how tightly the data points are clustered.
        Used for evaluating K-means clustering.
    Adjusted Rand Index (ARI): Compares the similarity of the actual clustering with a ground truth.
        Adjusts for chance, useful when you have labeled data.

# **7. What is model overfitting, and how can it be prevented?**
Overfitting occurs when a machine learning model learns not only the underlying patterns in the training data 
but also the noise and random fluctuations. As a result, the model performs very well on the training data but
poorly on unseen, test data because it has become too specific to the training set and lacks generalization.

Overfitting can be prevented implementing several strategies:
    Cross-Validation:
        Use techniques like k-fold cross-validation to evaluate the model on different subsets of the data, ensuring 
        that it performs well across various splits and is not just memorizing the training data.

    Simplify the Model:
        Reduce model complexity by selecting a simpler model or reducing the number of features.
        
    Regularization:
        Regularization techniques add a penalty to the model’s complexity to discourage overfitting.
      
    Use More Training Data:
        If possible, increase the amount of training data so the model has more examples to learn from, helping it generalize better.

    Early Stopping:
        In iterative models like neural networks, early stopping monitors the model's performance on a validation set and stops training
        when the performance starts to degrade, preventing the model from overfitting to the training data.

    Data Augmentation:
        For image or text data, data augmentation techniques generate additional training examples by slightly altering existing ones (e.g., rotating, flipping images), 
        which helps the model generalize better.

    Ensemble Methods:
        Combining the predictions of multiple models (e.g., Random Forest, Gradient Boosting) often leads to better generalization and reduces the risk of overfitting.
